{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TITLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Source:**\n",
    "\n",
    "freeCodeCamp.org : PyTorch for Deep Learning & Machine Learning â€“ Full Course \n",
    "\n",
    "This example is a example of implementation of classification example from the provided tutorial.\n",
    "\n",
    "**References:**\n",
    "- Link to resource: https://www.youtube.com/watch?v=V_xro1bcAuA\n",
    "- https://pytorch.org/tutorials/beginner/ptcheat.html\n",
    "- https://www.geeksforgeeks.org/machine-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01.01 Setting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install basic packages required for the project\n",
    "%conda install numpy pandas matplotlib\n",
    "%conda install pytorch -c pytorch\n",
    "%conda install scikit-learn\n",
    "%conda install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01.02 Importing Libraries to python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01.03 Setting up Device Agnostic code and device selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup devices agnostic code\n",
    "print(\"Setting up device agnostic ...\")\n",
    "if torch.cuda.is_available():       # Check if cuda available\n",
    "    device = torch.device(\"cuda\")   # Set device as cuda\n",
    "elif torch.mps.is_available():      # Check if mps available\n",
    "    device = torch.device(\"mps\")    # Set device as mps\n",
    "else:                               # Default device selection\n",
    "    device = torch.device(\"cpu\")    # Set device as cpu, Default behavour\n",
    "\n",
    "print(f\"Selected device for processing : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01.04 Setting up some Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):\n",
    "    \"\"\"Plots decision boundaries of model predicting on X in comparison to y.\n",
    "\n",
    "    Source - https://madewithml.com/courses/foundations/neural-networks/ (with modifications)\n",
    "    \"\"\"\n",
    "    # Put everything to CPU (works better with NumPy + Matplotlib)\n",
    "    model.to(\"cpu\")\n",
    "    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "    # Setup prediction boundaries and grid\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n",
    "\n",
    "    # Make features\n",
    "    X_to_pred_on = torch.from_numpy(np.column_stack((xx.ravel(), yy.ravel()))).float()\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_logits = model(X_to_pred_on)\n",
    "\n",
    "    # Test for multi-class or binary and adjust logits to prediction labels\n",
    "    if len(torch.unique(y)) > 2:\n",
    "        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # mutli-class\n",
    "    else:\n",
    "        y_pred = torch.round(torch.sigmoid(y_logits))  # binary\n",
    "\n",
    "    # Reshape preds and plot\n",
    "    y_pred = y_pred.reshape(xx.shape).detach().numpy()\n",
    "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "\n",
    "# Plot linear data or training and test and predictions (optional)\n",
    "def plot_predictions(\n",
    "    train_data, train_labels, test_data, test_labels, predictions=None\n",
    "):\n",
    "    \"\"\"\n",
    "  Plots linear training data and test data and compares predictions.\n",
    "  \"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    # Plot training data in blue\n",
    "    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
    "\n",
    "    # Plot test data in green\n",
    "    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "\n",
    "    if predictions is not None:\n",
    "        # Plot the predictions in red (predictions were made on the test data)\n",
    "        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "\n",
    "    # Show the legend\n",
    "    plt.legend(prop={\"size\": 14})\n",
    "\n",
    "# Plot loss curves of a model\n",
    "def plot_loss_curves(results):\n",
    "    \"\"\"Plots training curves of a results dictionary.\n",
    "\n",
    "    Args:\n",
    "        results (dict): dictionary containing list of values, e.g.\n",
    "            {\"train_loss\": [...],\n",
    "             \"train_acc\": [...],\n",
    "             \"test_loss\": [...],\n",
    "             \"test_acc\": [...]}\n",
    "    \"\"\"\n",
    "    loss = results[\"train_loss\"]\n",
    "    test_loss = results[\"test_loss\"]\n",
    "\n",
    "    accuracy = results[\"train_acc\"]\n",
    "    test_accuracy = results[\"test_acc\"]\n",
    "\n",
    "    epochs = range(len(results[\"train_loss\"]))\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label=\"train_loss\")\n",
    "    plt.plot(epochs, test_loss, label=\"test_loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label=\"train_accuracy\")\n",
    "    plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Data Preparation and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02.01 Preparing (Generating) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02.02 Train Test Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Build a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03.01 Setting up Helper Functions for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03.02 Setting up HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03.03 Writing the model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to reference the [ML Activation function cheatsheet website](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.00 Setting up helper methods for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from timeit import default_timer as timer \n",
    "\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format). \n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time\n",
    "\n",
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        Calculates accuracy between truth labels and predictions.\n",
    "\n",
    "        Args:\n",
    "            y_true (torch.Tensor): Truth labels for predictions.\n",
    "            y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
    "\n",
    "        Returns:\n",
    "            [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
    "    \"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "def logitsToPredictionActivationFn(logits, dim=1):\n",
    "    \"\"\"\n",
    "        Activation function that converts logits to prediction probabilities and then to prediction labels\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): Logits output from the model.\n",
    "            dim (int): Dimension along which to apply the softmax function.\n",
    "        \n",
    "        Returns:\n",
    "            [torch.int]: Prediction labels.\n",
    "    \"\"\"\n",
    "    return torch.softmax(logits, dim).argmax(dim)\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "                  X: torch.Tensor,\n",
    "                  y: torch.Tensor,\n",
    "                  loss_fn: torch.nn.Module,\n",
    "                  optimizer: torch.optim.Optimizer,\n",
    "                  device: torch.device = device) -> tuple:\n",
    "    \"\"\"\n",
    "        Training step for a model. \n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): Model to train. \n",
    "            x (torch.Tensor): Input Features. \n",
    "            y (torch.Tensor): Target Labels. \n",
    "            loss_fn (torch.nn.Module): Loss Function to use for training. \n",
    "            optimizer (torch.optim.Optimizer): Optimizer to use for training. \n",
    "            device (torch.device, optional): Device to train on. Defaults to device.\n",
    "        \n",
    "        Returns:\n",
    "            (torch.float, torch.float): Training Loss and Accuracy tuple. \n",
    "    \"\"\"\n",
    "    \n",
    "    # train_loss, train_acc = 0.0, 0.0\n",
    "    # Move model and data to device\n",
    "    model.to(device)\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    model.train()                                       # Set model to training mode\n",
    "    logits = model(X)                                   # Forward Pass through model\n",
    "    pred = logitsToPredictionActivationFn(logits)       # Apply softmax to get prediction probability and get prediction index\n",
    "    \n",
    "    train_loss = loss_fn(logits,y)                      # Calculate loss, with BCEWithLogitsLoss we use logits insted of pred\n",
    "    train_acc = accuracy_fn(y_true=y, y_pred=pred)      # Calculate accuracy\n",
    "\n",
    "    optimizer.zero_grad()                               # Clear gradients from previous iteration\n",
    "\n",
    "    train_loss.backward()                               # Back propogation\n",
    "\n",
    "    optimizer.step()                                    # Gradient Decent, update weights\n",
    "\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "              X: torch.Tensor,\n",
    "              y: torch.Tensor,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device = device) -> tuple:\n",
    "    \"\"\"\n",
    "        Make predictions on a test set and calculate accuracy\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): Model to train. \n",
    "            X (torch.Tensor): Input Features. \n",
    "            y (torch.Tensor): Target Labels. \n",
    "            loss_fn (torch.nn.Module): Loss Function to use for training.\n",
    "            device (torch.device, optional): Device to train on. Defaults to device.\n",
    "        Returns:\n",
    "            (torch.float, torch.float): Testing Loss and Accuracy tuple. \n",
    "    \"\"\"\n",
    "    test_loss, test_acc = 0.0, 0.0\n",
    "    # move model and daa to device\n",
    "    model.to(device)\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    model.eval()                                            # Set model to evaluation mode\n",
    "    with torch.inference_mode():                            # Disable gradient calculation for performance reasons\n",
    "        logits = model(X)                                   # Forward Pass through model\n",
    "        pred = logitsToPredictionActivationFn(logits)       # Apply softmax to get prediction probability and get prediction index\n",
    "        \n",
    "        test_loss = loss_fn(logits,y)                       # Calculate loss, as we use BCEWithLogitsLoss we use logits insted of pred\n",
    "        test_acc = accuracy_fn(y_true=y, y_pred=pred)       # Calculate accuracy\n",
    "    \n",
    "    return test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.01 Setting up Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different problem types require different loss functions. \n",
    "\n",
    "For example, for a regression problem (predicting a number) you might use mean absolute error (MAE) loss.\n",
    "\n",
    "And for a binary classification problem (like ours), you'll often use [binary cross entropy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) as the loss function.\n",
    "\n",
    "However, the same optimizer function can often be used across different problem spaces.\n",
    "\n",
    "For example, the stochastic gradient descent optimizer (SGD, `torch.optim.SGD()`) can be used for a range of problems, and the same applies to the Adam optimizer (`torch.optim.Adam()`). \n",
    "\n",
    "| Loss function/Optimizer | Problem type | PyTorch Code |\n",
    "| ----- | ----- | ----- |\n",
    "| Stochastic Gradient Descent (SGD) optimizer | Classification, regression, many others. | [`torch.optim.SGD()`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) |\n",
    "| Adam Optimizer | Classification, regression, many others. | [`torch.optim.Adam()`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) |\n",
    "| Binary cross entropy loss | Binary classification | [`torch.nn.BCELossWithLogits`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) or [`torch.nn.BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) |\n",
    "| Cross entropy loss | Multi-class classification | [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) |\n",
    "| Mean absolute error (MAE) or L1 Loss | Regression | [`torch.nn.L1Loss`](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html) | \n",
    "| Mean squared error (MSE) or L2 Loss | Regression | [`torch.nn.MSELoss`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) |  \n",
    "\n",
    "*Table of various loss functions and optimizers, there are more but these are some common ones you'll see.*\n",
    "\n",
    "Since we're working with a binary classification problem, let's use a binary cross entropy loss function.\n",
    "\n",
    "> **Note:** Recall a **loss function** is what measures how *wrong* your model predictions are, the higher the loss, the worse your model.\n",
    ">\n",
    "> Also, PyTorch documentation often refers to loss functions as \"loss criterion\" or \"criterion\", these are all different ways of describing the same thing.\n",
    "\n",
    "PyTorch has two binary cross entropy implementations:\n",
    "1. [`torch.nn.BCELoss()`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) - Creates a loss function that measures the binary cross entropy between the target (label) and input (features).\n",
    "2. [`torch.nn.BCEWithLogitsLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) - This is the same as above except it has a sigmoid layer ([`nn.Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)) built-in (we'll see what this means soon).\n",
    "\n",
    "Which one should you use? \n",
    "\n",
    "The [documentation for `torch.nn.BCEWithLogitsLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) states that it's more numerically stable than using `torch.nn.BCELoss()` after a `nn.Sigmoid` layer. \n",
    "\n",
    "So generally, implementation 2 is a better option. However for advanced usage, you may want to separate the combination of `nn.Sigmoid` and `torch.nn.BCELoss()` but that is beyond the scope of this notebook.\n",
    "\n",
    "Knowing this, let's create a loss function and an optimizer. \n",
    "\n",
    "For the optimizer we'll use `torch.optim.SGD()` to optimize the model parameters with learning rate 0.1.\n",
    "\n",
    "> **Note:** There's a [discussion on the PyTorch forums about the use of `nn.BCELoss` vs. `nn.BCEWithLogitsLoss`](https://discuss.pytorch.org/t/bceloss-vs-bcewithlogitsloss/33586/4). It can be confusing at first but as with many things, it becomes easier with practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.02 Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we've got a loss function and optimizer ready to go, let's train a model.\n",
    "\n",
    "Steps in training:\n",
    "\n",
    "<details>\n",
    "    <summary>PyTorch training loop steps</summary>\n",
    "    <ol>\n",
    "        <li><b>Forward pass</b> - The model goes through all of the training data once, performing its\n",
    "            <code>forward()</code> function\n",
    "            calculations (<code>model(x_train)</code>).\n",
    "        </li>\n",
    "        <li><b>Calculate the loss</b> - The model's outputs (predictions) are compared to the ground truth and evaluated\n",
    "            to see how\n",
    "            wrong they are (<code>loss = loss_fn(y_pred, y_train</code>).</li>\n",
    "        <li><b>Zero gradients</b> - The optimizers gradients are set to zero (they are accumulated by default) so they\n",
    "            can be\n",
    "            recalculated for the specific training step (<code>optimizer.zero_grad()</code>).</li>\n",
    "        <li><b>Perform backpropagation on the loss</b> - Computes the gradient of the loss with respect for every model\n",
    "            parameter to\n",
    "            be updated (each parameter\n",
    "            with <code>requires_grad=True</code>). This is known as <b>backpropagation</b>, hence \"backwards\"\n",
    "            (<code>loss.backward()</code>).</li>\n",
    "        <li><b>Step the optimizer (gradient descent)</b> - Update the parameters with <code>requires_grad=True</code>\n",
    "            with respect to the loss\n",
    "            gradients in order to improve them (<code>optimizer.step()</code>).</li>\n",
    "    </ol>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.03 Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Storing & Loading Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05.01 Saving Model state to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05.02 Loading Model state from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05.03 Testing loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
