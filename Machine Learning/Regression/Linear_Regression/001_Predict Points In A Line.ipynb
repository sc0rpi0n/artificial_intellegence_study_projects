{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict points in a Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Source:**\n",
    "\n",
    "freeCodeCamp.org : PyTorch for Deep Learning & Machine Learning â€“ Full Course \n",
    "\n",
    "This example is a example of implementation of linear regression from the provided tutorial.\n",
    "\n",
    "**References:**\n",
    "- Link to resource: https://www.youtube.com/watch?v=V_xro1bcAuA\n",
    "- https://pytorch.org/tutorials/beginner/ptcheat.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01.01 Setting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install basic packages required for the project\n",
    "%conda install numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install pytorch -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01.02 Importing Libraries to python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01.03 Setting up Device Agnostic code and device selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup devices agnostic code\n",
    "print(\"Setting up device agnostic ...\")\n",
    "if torch.cuda.is_available():       # Check if cuda available\n",
    "    device = torch.device(\"cuda\")   # Set device as cuda\n",
    "elif torch.mps.is_available():      # Check if mps available\n",
    "    device = torch.device(\"mps\")    # Set device as mps\n",
    "else:                               # Default device selection\n",
    "    device = torch.device(\"cpu\")    # Set device as cpu, Default behavour\n",
    "\n",
    "print(f\"Selected device for processing : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Data Preparation and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is a game of 2 parts:\n",
    "1. Get data into a numerical representation\n",
    "2. Build a model to learn patterns in the numerical representation\n",
    "\n",
    "\n",
    "To showcase this let's create some data using the linear regression formula\n",
    "\n",
    "Yi = f(Xi,B) + ei\n",
    "\n",
    "Yi = dependant variable\n",
    "f = function\n",
    "Xi = independent variable\n",
    "B = unknown parameters (beta)\n",
    "ei = error terms\n",
    "\n",
    "Well use a linear regression formula to make straight line with known **parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02.01 Preparing (Generating) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create some data using the linear formula y = m * x + c\n",
    "m = 0.7\n",
    "c = 0.3\n",
    "\n",
    "# Create range values already normalized to be between 0 and 1\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "\n",
    "# Create Features and Labels for our dataset\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1) # create a tensor of size [batch_size, 1] by unsqueezing a tensor of [batch_size] size dataset with 1 channel\n",
    "y = m * X + c   # Create our label data aka dependent variable y from the given Features ie the independent variable X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02.02 Train Test Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split our dataset into training and testing sets\n",
    "\n",
    "# We will use 80% of the data for training and 20% for testing\n",
    "train_split = int(0.8*len(X))\n",
    "\n",
    "# Split our dataset into train and test sets\n",
    "X_train, y_train = X[:train_split], y[:train_split] # Prepare our training data\n",
    "X_test, y_test = X[train_split:], y[train_split:]   # Prepare our testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize our datas in a plot. The plot function utilizes matplotlib to plot Traning and Testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing required packages for helper_functions.py\n",
    "%conda install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix issues: ImportError: cannot import name 'is_directory' from 'PIL._util' for torchvision\n",
    "%conda install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.helper_functions import plot_predictions, plot_loss_curves, accuracy_close_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Build a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will build a simple linear regression model that will inherit nn.Module. This is a basic implementation where we will only do some limited functionalities.  \n",
    "What our model does ?\n",
    "* Start with random values for parameters wt and bias\n",
    "* Look at training data and adjust the random values to better represent the ideal values.  \n",
    "We might have a question how will our model be able to acheave this, throught 2 main algorithm:\n",
    "    - Gradient decent\n",
    "    - Back Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch model building essentials\n",
    "\n",
    "* torch.nn - contains all of the building blocks for computational grapns ( a nn can be considered a computational graph)\n",
    "* torch.nn.Parameter - what parameters should model try and learn, ofter a pytorch layer  from torch.nn will set these for us\n",
    "* torch.nn.Module - The base class for all neural netowrk moduels, if you subclass it, you should overwrite forward()\n",
    "* torch.optim - This is where the optimizers in pytorch live, they will help with gradient descent\n",
    "* def forward() - All nn,Module subclasses require you to overwrite forward(), this method defines what happens in the forward computation\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/ptcheat.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class LinearRegressionBasicModelV1(nn.Module):                        # inherit nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We have only 1 feature in X and we want to predict Y so 1 output neuron\n",
    "        self.wt = nn.Parameter(torch.randn(1,                       # Initialize wt with random values, 1 is the number of features in our dataset, set it as parameter\n",
    "                                           requires_grad=True,      # True means that the gradient will be calculated for this parameter\n",
    "                                           dtype=torch.float))      # makesure the datatype is float for the wt parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(1,                     # Initialize bias with zeros, 1 is the number of features in our dataset, set it as parameter\n",
    "                                             requires_grad=True,    # True means that the gradient will be calculated for this parameter\n",
    "                                             dtype=torch.float))    # makesure the datatype is float for the bias parameter\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:             # Define the forward pass of our model, takes in a tensor and returns a tensor\n",
    "        '''\n",
    "           Forward pass through the network.\n",
    "           Args:\n",
    "               x (torch.Tensor): Input tensor of shape (batch_size, 1).\n",
    "           Returns:\n",
    "               torch.Tensor: Output tensor of shape (batch_size, 1).\n",
    "        '''\n",
    "        return self.wt * x + self.bias\n",
    "\n",
    "class LinearRegressionBasicModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We have only 1 feature in X and we want to predict Y so 1 output neuron\n",
    "        self.linear_layer = nn.Linear(in_features=1, out_features=1) # Define a linear layer with 1 input and 1 output\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Forward pass through the network.\n",
    "           Args:\n",
    "               x (torch.Tensor): Input tensor of shape (batch_size, 1).\n",
    "           Returns:\n",
    "               torch.Tensor: Output tensor of shape (batch_size, 1).\n",
    "        '''\n",
    "        return self.linear_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model_0 = LinearRegressionBasicModelV1()\n",
    "print(\"Linear Regression Model 0:\")\n",
    "print(\"Parameters:\")\n",
    "print(list(model_0.parameters()))\n",
    "print(\"StateDict:\")\n",
    "print(model_0.state_dict())\n",
    "\n",
    "model_1 = LinearRegressionBasicModelV2()\n",
    "print(\"Linear Regression Model 1:\")\n",
    "print(\"Parameters:\")\n",
    "print(list(model_1.parameters()))\n",
    "print(\"StateDict:\")\n",
    "print(model_1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the model to use the target device\n",
    "model_0.to(device)\n",
    "model_1.to(device)\n",
    "\n",
    "# put data on the target device\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction with model (predictions is called inference).\n",
    "# It does not track grads so less memory usage and faster as less things to keep track\n",
    "# You can also do something similar with torch.no_grad() but inference_mode is preferred\n",
    "with torch.inference_mode():\n",
    "    y_pred_0 = model_0(X_test)\n",
    "    y_pred_1 = model_1(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(X_train.cpu(), y_train.cpu(), X_test.cpu(), y_test.cpu(), predictions = y_pred_0.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(X_train.cpu(), y_train.cpu(), X_test.cpu(), y_test.cpu(), predictions = y_pred_1.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to move from some unknown parameters to know parameters ie from poor representation to better representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.01 Setting up HyperParameter, Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to measure how poor or how wring your models prediction are is to use a loss function, aka criterion or cost function.\n",
    "\n",
    "Things we need to train:\n",
    "\n",
    "* loss function: A func to measure how wrong our models prediction is.\n",
    "* Optimizer: takes into account the loss of a model and adjusts the model's param to improve the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01                                                   # learning rate\n",
    "loss_fn = nn.MSELoss()                                      # setting up loss function as mean squared error \n",
    "optimizer_0 = torch.optim.SGD(params=model_0.parameters(),  # SGD is a stochastic gradient descent optimizer\n",
    "                              lr=lr)\n",
    "\n",
    "optimizer_1 = torch.optim.SGD(params=model_1.parameters(),\n",
    "                          lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.02 Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a training & testing loop in Pytorch\n",
    "Okay, now we've got a loss function and optimizer ready to go, let's train a model.\n",
    "\n",
    "Steps in training:\n",
    "\n",
    "<details>\n",
    "    <summary>PyTorch training loop steps</summary>\n",
    "    <ol>\n",
    "        <li><b>Forward pass</b> - The model goes through all of the training data once, performing its\n",
    "            <code>forward()</code> function\n",
    "            calculations (<code>model(x_train)</code>).\n",
    "        </li>\n",
    "        <li><b>Calculate the loss</b> - The model's outputs (predictions) are compared to the ground truth and evaluated\n",
    "            to see how\n",
    "            wrong they are (<code>loss = loss_fn(y_pred, y_train</code>).</li>\n",
    "        <li><b>Zero gradients</b> - The optimizers gradients are set to zero (they are accumulated by default) so they\n",
    "            can be\n",
    "            recalculated for the specific training step (<code>optimizer.zero_grad()</code>).</li>\n",
    "        <li><b>Perform backpropagation on the loss</b> - Computes the gradient of the loss with respect for every model\n",
    "            parameter to\n",
    "            be updated (each parameter\n",
    "            with <code>requires_grad=True</code>). This is known as <b>backpropagation</b>, hence \"backwards\"\n",
    "            (<code>loss.backward()</code>).</li>\n",
    "        <li><b>Step the optimizer (gradient descent)</b> - Update the parameters with <code>requires_grad=True</code>\n",
    "            with respect to the loss\n",
    "            gradients in order to improve them (<code>optimizer.step()</code>).</li>\n",
    "    </ol>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model: LinearRegressionBasicModelV1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5000\n",
    "\n",
    "#track different metrics\n",
    "results_0 = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"test_acc\": []\n",
    "    }\n",
    "\n",
    "# Train model LinearRegressionBasicModelV1\n",
    "for epoch in range(epochs):\n",
    "    # Training Loop\n",
    "    model_0.train()                                 # Set model to training mode\n",
    "    y_pred_0 = model_0(X_train)                     # Forward Pass\n",
    "    train_loss = loss_fn(y_pred_0, y_train)         # Calculate training loss\n",
    "    train_acc = accuracy_close_fn(y_train,          # Calculate training accuracy\n",
    "                                  y_pred_0,\n",
    "                                  rtol=0.01,\n",
    "                                  atol=0.01)\n",
    "    optimizer_0.zero_grad()                         # Zero Grad\n",
    "    train_loss.backward()                           # Back Propogation\n",
    "    optimizer_0.step()                              # Gradient Descent\n",
    "\n",
    "    # Testing Loop\n",
    "    model_0.eval()                                  # Set model to evaluation mode\n",
    "    with torch.inference_mode():                    # Disable gradient calculation for performance reasons\n",
    "        test_pred = model_0(X_test)                 # Make Prediction using the trained model\n",
    "        test_loss = loss_fn(test_pred, y_test)      # Calculate model loss\n",
    "        test_acc = accuracy_close_fn(y_test,        # Calculate model accuracy\n",
    "                                     test_pred,\n",
    "                                     rtol=0.01,\n",
    "                                     atol=0.01)\n",
    "    \n",
    "    if epoch %10 == 0:\n",
    "        results_0[\"train_loss\"].append(train_loss.cpu().detach().numpy())\n",
    "        results_0['train_acc'].append(train_acc)\n",
    "        results_0[\"test_loss\"].append(test_loss.cpu().detach().numpy())\n",
    "        results_0[\"test_acc\"].append(test_acc)\n",
    "        print(f\"Epoch: {epoch} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model V1 Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(results=results_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(X_train.cpu(), y_train.cpu(), X_test.cpu(), y_test.cpu(), predictions = test_pred.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model: LinearRegressionBasicModelV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5000\n",
    "\n",
    "#track different metrics\n",
    "results_1 = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"test_acc\": []\n",
    "    }\n",
    "\n",
    "# Train model LinearRegressionBasicModelV2\n",
    "for epoch in range(epochs):\n",
    "    # Training Loop\n",
    "    model_1.train()                                 # Set model to training mode\n",
    "    y_pred_1 = model_1(X_train)                     # Forward Pass\n",
    "    train_loss = loss_fn(y_pred_1, y_train)         # Calculate training loss\n",
    "    train_acc = accuracy_close_fn(y_train,          # Calculate training accuracy\n",
    "                                  y_pred_1,\n",
    "                                  rtol=0.01,\n",
    "                                  atol=0.01)\n",
    "    optimizer_1.zero_grad()                         # Zero Grad\n",
    "    train_loss.backward()                           # Back Propogation\n",
    "    optimizer_1.step()                              # Gradient Descent\n",
    "\n",
    "    # Testing Loop\n",
    "    model_1.eval()                                  # Set model to evaluation mode\n",
    "    with torch.inference_mode():                    # Disable gradient calculation for performance reasons\n",
    "        test_pred = model_1(X_test)                 # Make Prediction using the trained model\n",
    "        test_loss = loss_fn(test_pred, y_test)      # Calculate model loss\n",
    "        test_acc = accuracy_close_fn(y_test,        # Calculate model accuracy\n",
    "                                     test_pred,\n",
    "                                     rtol=0.01,\n",
    "                                     atol=0.01)\n",
    "    \n",
    "    if epoch %10 == 0:\n",
    "        results_1[\"train_loss\"].append(train_loss.cpu().detach().numpy())\n",
    "        results_1['train_acc'].append(train_acc)\n",
    "        results_1[\"test_loss\"].append(test_loss.cpu().detach().numpy())\n",
    "        results_1[\"test_acc\"].append(test_acc)\n",
    "        print(f\"Epoch: {epoch} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model V2 Training Results Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(results=results_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(X_train.cpu(), y_train.cpu(), X_test.cpu(), y_test.cpu(), predictions = test_pred.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Storing & Loading Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For saving a model in Pytorch, there are 3 main methods are available we should know about saving and loading models\n",
    "\n",
    "1. `torch.save()` - allows you to save a Pytorch model in python pickle format.\n",
    "2. `torch.load()` - allows you to load a saved Pytorch obj\n",
    "3. `torch.nn.Module.load_state_dict()` - this allows you to save state dictionary\n",
    "\n",
    "\n",
    "\n",
    "You can either just save model dictionary or save and load whole model. Both has pros and cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05.01 Saving Model state to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "MODEL_PATH = Path(\"saved_models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_0_NAME = \"001_LinearRegressionBasicModelV1.pth\"\n",
    "MODEL_0_SAVE_PATH = MODEL_PATH / MODEL_0_NAME\n",
    "\n",
    "MODEL_1_NAME = \"001_LinearRegressionBasicModelV2.pth\"\n",
    "MODEL_1_SAVE_PATH = MODEL_PATH / MODEL_1_NAME\n",
    "\n",
    "print(f\"Saving model LinearRegressionBasicModelV1 to : {MODEL_0_SAVE_PATH}\")\n",
    "torch.save(obj=model_0.state_dict(),\n",
    "           f=MODEL_0_SAVE_PATH)\n",
    "print(\"Done\")\n",
    "\n",
    "print(f\"Saving model LinearRegressionBasicModelV2 to : {MODEL_1_SAVE_PATH}\")\n",
    "torch.save(obj=model_0.state_dict(),\n",
    "           f=MODEL_1_SAVE_PATH)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05.02 Loading Model state from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we saved model's `state_dict()` rather than entire model, we create a new instance of a model class and load state_dict() into that new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_0 = LinearRegressionBasicModelV1()                 # Create an instance of the model\n",
    "loaded_model_0.load_state_dict(torch.load(f=MODEL_0_SAVE_PATH)) # Load the saved state dict\n",
    "\n",
    "loaded_model_0.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05.03 Testing loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction from loaded model\n",
    "loaded_model_0.eval()\n",
    "with torch.inference_mode():\n",
    "  loaded_model_preds = loaded_model_0(X_test)\n",
    "\n",
    "#make prediction from old model\n",
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "  model_0_preds = model_0(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.eq(loaded_model_preds, model_0_preds).all()               # check if predictions are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we saw two different ways we can utilize pytorch to create a linear regression model. One method included defining parameters ourselves, while other utilizing inbuild nn.Linear as a layer in our custom model. We also looked at the different stages in building and training a model. We utilized device agnostic code to do our training calculation on gpu's where available. We also utilized MSE(Mean Squeared Error) loss function to calculate loss and SGD (Stochastic Gradient Decent) for the optimizer for gradient decent for a linear regression problem. Lastly we we looked at how to save our model in torch format, so that it can be loaded later on.  \n",
    "\n",
    "As this was a single independent variable ie simple linear regression example, there the model was simple in many ways including dimentional requirement for feature data sets. Next steps from this example would be moving on to building multiple independent variable regression model called multiple linear regression model.  \n",
    "\n",
    "Further steps to learn would be optimization of hyperparameters and using different algorithms like gradient descent, stochastic gradient descent etc and implementing on the build models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
